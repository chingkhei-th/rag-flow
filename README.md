# Context-Aware RAG System

A context-aware Retrieval-Augmented Generation (RAG) system built from scratch. This project is designed to ingest local documents (PDFs, Text, Markdown), generate vector embeddings, store them in a local ChromaDB instance, and answer user queries with precise source citations using local open-source LLMs via Ollama.

## üöÄ Features

- **Incremental Document Ingestion**: Automatically scans the `data/` directory and intelligently ingests *only* new or modified documents. It prevents duplicating data in your vector database, calculating the exact number of chunks stored.
- **Multi-Format Support**: Seamlessly processes and chunks `.pdf`, `.txt`, and `.md` files out of the box.
- **Robust High-Accuracy Embeddings**: Uses a custom `OllamaRobustEmbeddings` LangChain wrapper (`src/embeddings.py`) that efficiently communicates with the Ollama `/api/embed` endpoint to mitigate numerical instabilities (NaN errors) while preserving maximum context window accuracy.
- **Source Tracking & Citations**: Every answer generated by the LLM includes explicitly tracked sources, including the original document filename and page number, ensuring the information is verifiable.
- **Fully Configurable via Environment variables**: Easily tweak chunk sizes, overlaps, LLM models, and endpoints purely through the `.env` file without altering code.

## üõ†Ô∏è Technology Stack

- **Python Environment**: Managed entirely by [uv](https://github.com/astral-sh/uv)
- **Framework**: [LangChain](https://python.langchain.com/) (using pure LCEL `langchain_core`)
- **Vector Database**: [ChromaDB](https://www.trychroma.com/)
- **Embedding & Generation Models**: [Ollama](https://ollama.ai/) (Default configured for `nomic-embed-text` and `qwen2.5:7b`)

## üìã Prerequisites

1. **Python 3.10+**
2. **uv Package Manager**: Install via `curl -LsSf https://astral.sh/uv/install.sh | sh` (Linux/macOS) or `powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"` (Windows).
3. **Ollama**: Running locally or on a remote server. Ensure you have the required models pulled:
   ```bash
   ollama pull nomic-embed-text:latest
   ollama pull qwen2.5:7b
   ```

## ‚öôÔ∏è Installation & Setup

1. **Clone the repository and enter the directory**:
   ```bash
   git clone <your-repo-url>
   cd rag
   ```

2. **Configure your environment**:
   Create a `.env` file in the root directory:
   ```env
   # .env
   OLLAMA_BASE_URL=http://localhost:11434    # Or your remote Ollama IP
   EMBEDDING_MODEL=nomic-embed-text:latest
   LLM_MODEL=qwen2.5:7b
   CHUNK_SIZE=500
   CHUNK_OVERLAP=50
   DATA_DIR=data
   CHROMA_DB_DIR=chroma_db
   ```

3. **Add your documents**:
   Create a directory named `data/` at the root of the project and place your initial `.pdf`, `.txt`, or `.md` files inside it.

## üéØ Usage

To run the RAG pipeline, use `uv` to automatically resolve dependencies and launch the CLI application:

```bash
uv run python main.py
```

**Upon startup, the system will:**
1. Initialize the embedding model and vector database connection.
2. Scan `data/` for unsupported or new files.
3. Compare file metadata against existing records in `ChromaDB`.
4. Parse, chunk, and embed only the newly detected files.
5. Open an interactive CLI chat terminal.

### Example Interaction

```
Initializing Advanced Context-Aware RAG Pipeline...

Starting document ingestion process...
Number of existing documents in DB: 0
Adding/Seeding new documents (Found 1 new files)
  - Document: sample.pdf | Number of chunks: 42
Successfully seeded all new documents.

==================================
RAG System Ready. Type 'exit' to quit.
==================================

You: What are the main objectives of the project mentioned in the summit?
Thinking...

AI: The main objective of the project is to foster grassroots AI integration to improve global employability and create localized impact within NE India.

Sources:
 - sample.pdf (Page: 2)
----------------------------------------
```

## üìÇ Project Structure

```text
‚îú‚îÄ‚îÄ .env                  # Configuration variables
‚îú‚îÄ‚îÄ .gitignore            # Ignored files/folders
‚îú‚îÄ‚îÄ main.py               # Application entry point and interactive CLI
‚îú‚îÄ‚îÄ README.md             # This documentation
‚îú‚îÄ‚îÄ data/                 # Directory containing raw input documents
‚îú‚îÄ‚îÄ chroma_db/            # Local SQLite-backed vector database storage
‚îî‚îÄ‚îÄ src/
    ‚îú‚îÄ‚îÄ config.py         # Logic for parsing and exporting environment variables
    ‚îú‚îÄ‚îÄ embeddings.py     # Custom Robust Embeddings wrapper for Ollama
    ‚îî‚îÄ‚îÄ rag_pipeline.py   # Core Retrieval, Augmentation, and Generation logic
```

## üêõ Troubleshooting

- **`ModuleNotFoundError` during import**: Ensure you are using `uv run python main.py` rather than your system python, so that the isolated `.venv` is used contextually.
- **`chromadb.errors.InvalidArgumentError`**: This happens if you initially populated your vector database using one model, and later changed `EMBEDDING_MODEL` in your `.env` to a different model (e.g., changing from a 1024 to a 768 dim model). **Solution**: Delete the existing `chroma_db` folder to let the system rebuild it from scratch:
  ```powershell
  Remove-Item -Recurse -Force .\chroma_db
  ```
- **`NaN` Value Error from Ollama**: If you revert to a less stable embedding model like `bge-m3:567m`, you may encounter a JSON encoder ValueError representing `NaN`. Use the stable `nomic-embed-text:latest` model to resolve this.
